{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Fine Tuning Bert using Skin Cancer Data</center>\n",
    "\n",
    "The code in this notebook is adopted from: https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=IUM0UA1qJaVB\n",
    "\n",
    "Skin cancer data (big and little) can be found in the wiki: https://knowledge.depaul.edu/display/DNLP/Tasks+and+Data\n",
    "\n",
    "For this notebook, TensorFlow 1.15 is required\n",
    "\n",
    "Date: 24 August 2020\n",
    "This is a collaboration between Keith Cochran and Clayton Cohn where Skin Cancer essays can be classified.  This builds on the work from Simon Hughes involving causal reasoning chains. {Doctoral Dissertation: \"Automatic Inference of Causal Reasoning Chains from Student Essays\", 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2391,
     "status": "ok",
     "timestamp": 1587144224293,
     "user": {
      "displayName": "Clayton Cohn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiR5T0VrYZ_A0_satvSE1jZbcVxmApPyylw8Q-uxoo=s64",
      "userId": "10103672487987981310"
     },
     "user_tz": 300
    },
    "id": "pxGVOeMQOAfs",
    "outputId": "ea10eee9-a1aa-4fe0-883e-40075d3ce079"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version 1.5.1\n",
      "Tensorflow version 1.15.0\n",
      "pandas version 1.0.5.\n",
      "numpy version 1.18.5.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "from pytorch_pretrained_bert.convert_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skopt import dump\n",
    "from skopt import gp_minimize\n",
    "from skopt import load\n",
    "from skopt.plots import plot_evaluations\n",
    "from skopt.plots import plot_objective\n",
    "from skopt.space import Categorical\n",
    "from skopt.space import Integer\n",
    "from skopt.space import Real\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.set_cmap(\"viridis\")\n",
    "\n",
    "# If using Google Colab, uncomment this line to make matplotlib inline\n",
    "#% matplotlib inline\n",
    "\n",
    "print(\"Torch version {}\".format(torch.__version__))\n",
    "print(\"Tensorflow version {}\".format(tf.__version__))\n",
    "print('pandas version {}.'.format(pd.__version__))\n",
    "print('numpy version {}.'.format(np.__version__))\n",
    "\n",
    "# Get date and time\n",
    "import datetime\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>To use the GPU, do the following</center>\n",
    "\n",
    "<table><tr><th>Environment</th><th>Instruction</th></tr><tr><td>Colab</td><td>Edit->Notebook Settings->Hardware Accelerator and select GPU</td></tr>\n",
    "    <tr><td>ML PC</td><td>Device is found using the provided libraries</td></tr></table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10509,
     "status": "ok",
     "timestamp": 1587144232544,
     "user": {
      "displayName": "Clayton Cohn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiR5T0VrYZ_A0_satvSE1jZbcVxmApPyylw8Q-uxoo=s64",
      "userId": "10103672487987981310"
     },
     "user_tz": 300
    },
    "id": "HTpUC6-8UKsN",
    "outputId": "0526d979-63af-4726-f2e6-5428dae4951a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "Cuda Device: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print('Cuda Device: {}'.format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Manual Parameters</center>\n",
    "\n",
    "| Hyper Parameter | Recommended Values         \n",
    "| :- | :-------------\n",
    "|EPOCHS| 2, 3, 4\n",
    "|BATCH_SIZE| 16, 32\n",
    "|MAX_LEN|Length of longest sentence\n",
    "|LEARNING_RATE|2e-5, 3e-5, 5e-5\n",
    "|WARMUP|.1\n",
    "|MODEL_PATH| The path to the model to use (i.e. 'bert-base-uncased')\n",
    "|COST_SENSITIVITY|0 if unused\n",
    "|KFOLD|0 or a value for kfold cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "reOH412ibrsp"
   },
   "outputs": [],
   "source": [
    "# define the various dimensions we want to test the range of to find the optimal set.\n",
    "dimensions = [\n",
    "    Real(low=2e-5, high=5e-5, prior='log-uniform', name='learning_rate'),\n",
    "]\n",
    "\n",
    "# To modify values here, uncomment the value desired and comment the others with the same name.\n",
    "EPOCHS = [2, 3, 4]\n",
    "BATCH_SIZE = [16, 32]\n",
    "MAX_LEN = 128\n",
    "WARMUP = 0.1\n",
    "MODEL_PATH_PREFIX = 'pre-trained_models/'\n",
    "MODEL_PATH = ['bert_base_uncased', \n",
    "              'scibert_scivocab_uncased', \n",
    "              'biobert_v1.1_pubmed']\n",
    "MODEL_NAME = ['BERT', 'SCIBERT', 'BIOBERT']\n",
    "DATA_PATH = \"data\"\n",
    "STATS_PATH = \"stats\"\n",
    "PRETRAINING_MODEL_ID = \"none\"\n",
    "PRETRAINING_MODEL_TYPE = \"none\"\n",
    "COST_SENSITIVITY = 0\n",
    "KFOLD = 0\n",
    "\n",
    "DATA_TYPE =  ['skin', 'coral']\n",
    "NOTES =      ['Skin Cancer', 'Coral Bleaching']\n",
    "STATS_FILE = ['skin_cancer_fine_tuning_stats.csv', 'coral_bleaching_fine_tuning_stats.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookData(object):\n",
    "    \n",
    "    def __init__(self, epochs=None, batch_size=None, max_len=0, warmup=0.0, model_path_prefix=\"\", \\\n",
    "               model_path=None, model_name=None, data_path=\"\", stats_path=\"\", pretraining_model_id=\"\", \\\n",
    "               pretraining_model_type=\"\", cost_sensitivity=0, kfold=0, data_type=None, notes=None, stats_file=None):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.warmup = warmup\n",
    "        self.model_path_prefix = model_path_prefix\n",
    "        self.model_path = model_path\n",
    "        self.model_name = model_name\n",
    "        self.data_path = data_path\n",
    "        self.stats_path = stats_path\n",
    "        self.pretraining_model_id = pretraining_model_id\n",
    "        self.pretraining_model_type = pretraining_model_type\n",
    "        self.cost_sensitivity = cost_sensitivity\n",
    "        self.kfold = kfold\n",
    "        self.data_type = data_type\n",
    "        self.notes = notes\n",
    "        self.stats_file = stats_file\n",
    "        \n",
    "    # prints when referred to\n",
    "    def __repr__(self):\n",
    "        return \"\"\n",
    "    \n",
    "    # prints when converted directly to string\n",
    "    def __str__(self):\n",
    "        retStr = \"\"\n",
    "        if (self.epochs != None):\n",
    "            retStr += \"epochs: \" + str(self.epochs)\n",
    "        if (self.batch_size != None):\n",
    "            retStr += \"\\nbatch_size: \" + str(self.batch_size)\n",
    "        retStr += \"\\nmax_len: \" + str(self.max_len)\n",
    "        retStr += \"\\nwarmup: \" + str(self.warmup)\n",
    "        retStr += \"\\nmodel_path_prefix: \" + str(self.model_path_prefix)\n",
    "        if (self.model_path != None):\n",
    "            retStr += \"\\nmodel_path: \" +  str(self.model_path)\n",
    "        retStr += \"\\nmodel_name: \" + str(self.model_name)\n",
    "        retStr += \"\\ndata_path: \" + str(self.data_path)\n",
    "        retStr += \"\\nstats_path: \" + str(self.stats_path)\n",
    "        retStr += \"\\npretraining_model_id: \" + str(self.pretraining_model_id)\n",
    "        retStr += \"\\npretraining_model_type: \" + str(self.pretraining_model_type) \n",
    "        retStr += \"\\ncost_sensitivity: \" + str(self.cost_sensitivity)\n",
    "        retStr += \"\\nkfold: \" + str(self.kfold)\n",
    "        if (self.data_type != None):\n",
    "            retStr += \"\\ndata_type: \" + str(self.data_type)\n",
    "        if (self.notes != None):\n",
    "            retStr += \"\\nnotes: \" + str(self.notes)\n",
    "        if (self.stats_file != None):\n",
    "            retStr += \"\\nstats_file: \" + str(self.stats_file)\n",
    "            \n",
    "        return retStr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: [2, 3, 4]\n",
      "batch_size: [16, 32]\n",
      "max_len: 128\n",
      "warmup: 0.1\n",
      "model_path_prefix: pre-trained_models/\n",
      "model_path: ['bert_base_uncased', 'scibert_scivocab_uncased', 'biobert_v1.1_pubmed']\n",
      "model_name: ['BERT', 'SCIBERT', 'BIOBERT']\n",
      "data_path: data\n",
      "stats_path: stats\n",
      "pretraining_model_id: none\n",
      "pretraining_model_type: none\n",
      "cost_sensitivity: 0\n",
      "kfold: 0\n",
      "data_type: ['skin', 'coral']\n",
      "notes: ['Skin Cancer', 'Coral Bleaching']\n",
      "stats_file: ['skin_cancer_fine_tuning_stats.csv', 'coral_bleaching_fine_tuning_stats.csv']\n"
     ]
    }
   ],
   "source": [
    "notebook_data = NotebookData(epochs = [2, 3, 4], batch_size = [16, 32], max_len = 128, warmup = 0.1, \\\n",
    "    model_path_prefix = 'pre-trained_models/', \\\n",
    "    model_path = ['bert_base_uncased', 'scibert_scivocab_uncased', 'biobert_v1.1_pubmed'], \\\n",
    "    model_name = ['BERT', 'SCIBERT', 'BIOBERT'], data_path = \"data\", stats_path = \"stats\", pretraining_model_id = \"none\", \\\n",
    "    pretraining_model_type = \"none\", data_type = ['skin', 'coral'], notes = ['Skin Cancer', 'Coral Bleaching'], \\\n",
    "    stats_file = ['skin_cancer_fine_tuning_stats.csv', 'coral_bleaching_fine_tuning_stats.csv'])\n",
    "\n",
    "print(notebook_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16306,
     "status": "ok",
     "timestamp": 1587144238396,
     "user": {
      "displayName": "Clayton Cohn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiR5T0VrYZ_A0_satvSE1jZbcVxmApPyylw8Q-uxoo=s64",
      "userId": "10103672487987981310"
     },
     "user_tz": 300
    },
    "id": "Y8fXOdJIULEO",
    "outputId": "85fdb01a-a3a6-49b0-cdea-a923ba4213eb"
   },
   "source": [
    "Make sure PyTorch is installed - will use with Hugging Face Transformers\n",
    "<br>Hugging Face library currently accepted as most powerful PyTorch interface with BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pretrained-bert in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: pytorch-nlp in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from pytorch-pretrained-bert) (4.31.1)\n",
      "Requirement already satisfied: torch>=0.4.1 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from pytorch-pretrained-bert) (1.5.1)\n",
      "Requirement already satisfied: requests in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from pytorch-pretrained-bert) (2.21.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from pytorch-pretrained-bert) (1.18.5)\n",
      "Requirement already satisfied: boto3 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from pytorch-pretrained-bert) (1.14.21)\n",
      "Requirement already satisfied: regex in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from pytorch-pretrained-bert) (2017.4.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from pytorch-nlp) (1.0.5)\n",
      "Requirement already satisfied: future in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.18.2)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2020.6.20)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.21 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.17.21)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from pandas->pytorch-nlp) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from pandas->pytorch-nlp) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from botocore<1.18.0,>=1.17.21->boto3->pytorch-pretrained-bert) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\keith\\anaconda3\\envs\\pythongpu\\lib\\site-packages (from python-dateutil>=2.6.1->pandas->pytorch-nlp) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# Install\n",
    "!pip install pytorch-pretrained-bert pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def get_tokenizer():\n",
    "    global tokenizer\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must transform relation labels to binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_relation_label_to_binary(df):\n",
    "    df_binary = df.copy(deep=True)\n",
    "    relations_pd = df.relation.copy(deep=True)\n",
    "\n",
    "    coral_relations = [\n",
    "                       \"1,2\", \"1,3\", \"1,4\", \"1,5\", \"1,5B\", \"1,14\", \"1,6\", \"1,7\", \"1,50\",\n",
    "                       \"2,3\", \"2,4\", \"2,5\", \"2,5B\", \"2,14\", \"2,6\", \"2,7\", \"2,50\",\n",
    "                       \"3,4\", \"3,5\", \"3,5B\", \"3,14\", \"3,6\", \"3,7\", \"3,50\",\n",
    "                       \"4,5\", \"4,5B\", \"4,14\", \"4,6\", \"4,7\", \"4,50\",\n",
    "                       \"5,5B\", \"5,14\", \"5,6\", \"5,7\", \"5,50\",\n",
    "                       \"5B,14\", \"5B,6\", \"5B,7\", \"5B,50\",\n",
    "                       \"11,12\", \"11,13\", \"11,14\", \"11,6\", \"11,7\", \"11,50\",\n",
    "                       \"12,13\", \"12,14\", \"12,6\", \"12,7\", \"12,50\",\n",
    "                       \"13,14\", \"13,6\", \"13,7\",\"13,50\",\n",
    "                       \"14,6\", \"14,7\", \"14,50\",\n",
    "                       \"6,7\", \"6,50\",\n",
    "                       \"7,50\"\n",
    "                      ]\n",
    "    #print(\"{} unique coral bleaching relations\".format(len(coral_relations)))\n",
    "\n",
    "    skin_relations = [\n",
    "                      \"1,2\", \"1,3\", \"1,4\", \"1,5\", \"1,6\", \"1,50\",\n",
    "                      \"2,3\", \"2,4\", \"2,5\", \"2,6\", \"2,50\",\n",
    "                      \"3,4\", \"3,5\", \"3,6\", \"3,50\",\n",
    "                      \"4,5\", \"4,6\", \"4,50\",\n",
    "                      \"5,6\", \"5,50\",\n",
    "                      \"11,12\", \"11,6\", \"11,50\",\n",
    "                      \"12,6\", \"12,50\",\n",
    "                      \"6,50\"     \n",
    "                     ]\n",
    "\n",
    "    #print(\"{} unique skin cancer relations\".format(len(skin_relations)))\n",
    "\n",
    "    for i, rel in relations_pd.items():\n",
    "        chain = rel.split(\"-\")\n",
    "        if chain[0] != \"O\":\n",
    "            chain = chain[1] + \",\" + chain[2]\n",
    "\n",
    "            if DATA_TYPE == \"coral\":\n",
    "                if chain in coral_relations:\n",
    "                    relations_pd.at[i] = 1\n",
    "                    continue\n",
    "\n",
    "            elif DATA_TYPE == \"skin\":\n",
    "                if chain in skin_relations:\n",
    "                    relations_pd.at[i] = 1\n",
    "                    continue\n",
    "\n",
    "        relations_pd.at[i] = 0\n",
    "\n",
    "    df_binary.relation = relations_pd\n",
    "    return df_binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must address the issue that some sentences have multiple relations. This could be a problem if a sentence has one valid relation and one invalid one (the same sentence will be labeled True in one instance and False in another instance). To correct this, we will remove the duplicate instances and define each sentence to be True if it contains *at least one* causal relation.\n",
    "\n",
    "The parse was provided by @TrentonMcKinney on StackOverflow:\n",
    "https://stackoverflow.com/questions/63697275/regex-string-for-different-versions/63697498#63697498"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the duplicates are isolated, they need to be evaluated. If there is at least one relation, one copy of the sentence will be kept as true. If there are no relations, one copy will be kept as false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df_binary):\n",
    "    df_duplicate_sentences = df_binary[df_binary.s_num.astype(str).str.split('.', expand=True)[1] != '0']\n",
    "    current = -1\n",
    "    same_arr_inds = []\n",
    "    drop_list = []\n",
    "\n",
    "    for i, row in df_duplicate_sentences.iterrows():\n",
    "        s_num = str(df_duplicate_sentences.loc[i].s_num)\n",
    "        first_num, second_num = s_num.split(\".\")\n",
    "\n",
    "        if first_num != current:\n",
    "            current = first_num\n",
    "\n",
    "        if len(same_arr_inds) > 1:\n",
    "            flag = False\n",
    "            for n in same_arr_inds:\n",
    "                if df_duplicate_sentences.loc[n].relation == True:\n",
    "                    flag = True\n",
    "                    break\n",
    "\n",
    "            left = same_arr_inds[0]\n",
    "            right = same_arr_inds[1:]\n",
    "\n",
    "            if flag == True:\n",
    "                df_duplicate_sentences.loc[left].relation = 1\n",
    "            else:\n",
    "                df_duplicate_sentences.loc[left].relation = 0\n",
    "\n",
    "            drop_list += right   \n",
    "\n",
    "            same_arr_inds = []\n",
    "            same_arr_inds.append(i)\n",
    "\n",
    "    df_binary.drop(drop_list, inplace=True)\n",
    "    \n",
    "    return df_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_binary(df):\n",
    "    return remove_duplicates(transform_relation_label_to_binary(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD: get_sentences_from_file()\n",
    "# Purpose:\n",
    "#  To obtain the sentences from a given file, depending on the data set intended and if it's large or small sentences.\n",
    "#\n",
    "# Parameters:\n",
    "# -data_type: ['skin', 'coral']\n",
    "# -sentence_type: ['big', 'little']\n",
    "#\n",
    "# Output:\n",
    "# -df: data frame\n",
    "# -sentences: data ready for BERT input\n",
    "# -tokenized_texts: sentences from the tokenizer\n",
    "#\n",
    "def get_sentences_from_file(data_type, sentence_type):\n",
    "    global notebook_data\n",
    "    filename = \"\"\n",
    "\n",
    "    if data_type == \"skin\":\n",
    "        if sentence_type == 'big':\n",
    "            filename = \"EBA1415-SkinCancer-big-sentences.tsv\"\n",
    "        elif sentence_type == 'little':\n",
    "            filename = \"EBA1415-SkinCancer-little-sentences.tsv\"\n",
    "        else:\n",
    "            print(\"sentence_type must be set to either 'big' or 'little.'\")\n",
    "    elif data_type == \"coral\":\n",
    "        if sentence_type == 'big':\n",
    "            filename = \"EBA1415-CoralBleaching-big-sentences.tsv\"\n",
    "        elif sentence_type == 'little':\n",
    "            filename = \"EBA1415-CoralBleaching-little-sentences.tsv\"\n",
    "        else:\n",
    "            print(\"sentence_type must be set to either 'big' or 'little.'\")\n",
    "    else:\n",
    "        print(\"data_type must be set to either 'coral' or 'skin.'\")\n",
    "\n",
    "    h = 0 if data_type == \"skin\" else None\n",
    "    df = pd.read_csv(notebook_data.data_path + \"/\" + filename, delimiter='\\t', header=h, \\\n",
    "                     names=['essay', 'relation', 's_num', 'sentence'])\n",
    "\n",
    "    # Create sentence and label lists\n",
    "    sentences_untokenized = df.sentence.values\n",
    "\n",
    "    # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences_untokenized]\n",
    "    tokenized_texts = [tokenizer.tokenize(sentence) for sentence in sentences_untokenized]\n",
    "    \n",
    "    print(\"get_sentences_from_file:Sentences found: {}\".format(len(sentences)))\n",
    "    df_binary = transform_to_binary(df)\n",
    "    \n",
    "    return df_binary, sentences, tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Zf9TEVWWUow",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_sentences_from_file:Sentences found: 10045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>relation</th>\n",
       "      <th>s_num</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EBA1415_TFHC_1_SC_ES-05947</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This essay is about skin damage, latitude and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EBA1415_TFHC_1_SC_ES-05947</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The skin damage is on our bodies that have num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EBA1415_TFHC_1_SC_ES-05947</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>There are three main varieties of skin cancer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EBA1415_TFHC_1_SC_ES-05947</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>That would be what skin damage is.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EBA1415_TFHC_1_SC_ES-05947</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Latitude and direct sunlight would be the cols...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EBA1415_TFHC_1_SC_ES-05947</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>The most yearound direct sunlight occurs betwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EBA1415_TFHC_1_SC_ES-05947</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>That would be latitude and direct sunlight.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EBA1415_TFHC_1_SC_ES-05947</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Your skin protects you is that it acts as a wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EBA1415_TFHC_1_SC_ES-05947</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Your skin does have some denses against solar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EBA1415_TFHC_1_SC_ES-05947</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>That would be your skin protects you.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        essay relation  s_num  \\\n",
       "0  EBA1415_TFHC_1_SC_ES-05947        0    1.0   \n",
       "1  EBA1415_TFHC_1_SC_ES-05947        0    2.0   \n",
       "2  EBA1415_TFHC_1_SC_ES-05947        0    3.0   \n",
       "3  EBA1415_TFHC_1_SC_ES-05947        0    4.0   \n",
       "4  EBA1415_TFHC_1_SC_ES-05947        0    5.0   \n",
       "5  EBA1415_TFHC_1_SC_ES-05947        0    6.0   \n",
       "6  EBA1415_TFHC_1_SC_ES-05947        0    7.0   \n",
       "7  EBA1415_TFHC_1_SC_ES-05947        0    8.0   \n",
       "8  EBA1415_TFHC_1_SC_ES-05947        0    9.0   \n",
       "9  EBA1415_TFHC_1_SC_ES-05947        0   10.0   \n",
       "\n",
       "                                            sentence  \n",
       "0  This essay is about skin damage, latitude and ...  \n",
       "1  The skin damage is on our bodies that have num...  \n",
       "2  There are three main varieties of skin cancer ...  \n",
       "3                 That would be what skin damage is.  \n",
       "4  Latitude and direct sunlight would be the cols...  \n",
       "5  The most yearound direct sunlight occurs betwe...  \n",
       "6        That would be latitude and direct sunlight.  \n",
       "7  Your skin protects you is that it acts as a wa...  \n",
       "8  Your skin does have some denses against solar ...  \n",
       "9              That would be your skin protects you.  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, sentences, tokenized_texts = get_sentences_from_file('skin', 'big')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is prepped and cleaned at this point. Next is implementation. Extract sentences and labels from DataFrame. Must also add special [CLS] and [SEP] tokens for BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sentences for BERT.\n",
    "For each tokenized input sentence, we need to create:\n",
    "\n",
    "1. input ids:\n",
    "    a sequence of integers identifying each input token to its index number \n",
    "    in the BERT tokenizer vocabulary\n",
    "\n",
    "2. segment mask: (optional) a sequence of 1s and 0s used to identify whether the input is one \n",
    "    sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. \n",
    "    For two sentence inputs, there is a 0 for each token of the first sentence, followed by a \n",
    "    1 for each token of the second sentence\n",
    "\n",
    "3. attention mask: (optional) \n",
    "    a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens \n",
    "\n",
    "4. labels: based on the labels from the data set\n",
    "\n",
    "Additionally, we will get rid of the sentences greater than MAX_LEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(tokenizer, df_binary, sentences, tokenized_texts):\n",
    "    original_length = len(tokenized_texts)\n",
    "    labels = df_binary.relation.values\n",
    "    labels = [labels[i] for i in range(len(tokenized_texts)) if len(tokenized_texts[i]) <= MAX_LEN]\n",
    "    tokenized_texts = [tokenized_texts[i] for i in range(len(tokenized_texts)) if len(tokenized_texts[i]) <= MAX_LEN]\n",
    "    print(\"Removed {0} sentences greater than {1}\".format(original_length - len(tokenized_texts),MAX_LEN))\n",
    "    \n",
    "    # Convert BERT tokens to corresponding ID numbers in BERT vocabulary. After conversion, pad the sequences.\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    # Create attention masks.\n",
    "    attention_masks = []\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    return input_ids, labels, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GANGNKH7jbd2"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqqQCDEWs0oN"
   },
   "outputs": [],
   "source": [
    "def getLastModelNumber(stats_file):\n",
    "    try:\n",
    "        with open(STATS_PATH + \"/\" + stats_file, \"r\") as f:\n",
    "            f_list = list(f)\n",
    "            latest = f_list[-1].split(',')\n",
    "            return int(latest[0])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_classification_dict(labels, classification_dict):\n",
    "    classification_dict_stripped = {}\n",
    "\n",
    "    # Create arrays of precisions, recalls, f1s to recalculate average\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    supports = []\n",
    "\n",
    "    # First we are going to create an array of all possible labels\n",
    "    label_types = {}\n",
    "    for i in range(len(labels)):\n",
    "        if str(labels[i]) not in label_types:\n",
    "            label_types.update({str(labels[i]) : len(label_types)})\n",
    "\n",
    "    for k,v in classification_dict.items():\n",
    "        if k.isdigit():\n",
    "            if float(v['precision']) != 0.0 and float(v['recall']) != 0.0 and float(v['f1-score']) != 0.0:\n",
    "                recalls.append(v['recall'])\n",
    "                f1s.append(v['f1-score'])\n",
    "                precisions.append(v['precision'])\n",
    "                supports.append(v['support'])\n",
    "\n",
    "                # Convert dictionary keys back to our original labels\n",
    "                original_key = next((key for key in label_types if label_types[key] == int(k)), None)\n",
    "                classification_dict_stripped.update({ original_key : v })\n",
    "\n",
    "        # Otherwise, we are at the end of the dict and edit the averages to account for the newly replaced 0s\n",
    "        else:\n",
    "            if k == 'macro avg':\n",
    "                precision = sum(precisions)/len(precisions)\n",
    "                recall = sum(recalls)/len(recalls)\n",
    "                f1 = sum(f1s)/len(f1s)\n",
    "\n",
    "                v['precision'] = precision\n",
    "                v['recall'] = recall\n",
    "                v['f1-score'] = f1\n",
    "\n",
    "            if k == 'weighted avg':\n",
    "                weighted_precisions = [precisions[i]*supports[i] for i in range(len(precisions))]\n",
    "                weighted_recalls = [recalls[i]*supports[i] for i in range(len(recalls))]\n",
    "                weighted_f1s = [f1s[i]*supports[i] for i in range(len(f1s))]\n",
    "\n",
    "                total_supports = sum(supports)\n",
    "\n",
    "                precision = sum(weighted_precisions)/total_supports\n",
    "                recall = sum(weighted_recalls)/total_supports\n",
    "                f1 = sum(weighted_f1s)/total_supports\n",
    "\n",
    "                v['precision'] = precision\n",
    "                v['recall'] = recall\n",
    "                v['f1-score'] = f1\n",
    "                v['support'] = total_supports\n",
    "\n",
    "            classification_dict_stripped.update({ k : v })\n",
    "    return classification_dict_stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_open_stats_file(STATS_PATH, stats_file):\n",
    "    f = None\n",
    "    if not os.path.isfile(STATS_PATH + \"/\" + stats_file):\n",
    "        f = open(STATS_PATH + \"/\" + stats_file, \"w\")\n",
    "        f.write(\"number,datetime,bert_model,hugging_face,max_len,epochs,batch_size,\\\n",
    "            optimizer,learning_rate,warmup,pretraining_model_id,pretraining_model_type,\\\n",
    "            cost_sensitivity,accuracy,macro_prec,macro_recall,macro_f1,macro_support,\\\n",
    "            weighted_prec,weighted_recall,weighted_f1,weighted_support,kfold,notes\\n\")\n",
    "        print(\"Stats file \" + stats_file + \" NOT found - creating\")\n",
    "        f.close()\n",
    "        \n",
    "    f = open(STATS_PATH + \"/\" + stats_file, \"a\")\n",
    "        \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(prediction_dataloader, model):\n",
    "    global device\n",
    "    predictions_test = []\n",
    "    true_labels_test = []\n",
    "\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids = torch.tensor(b_input_ids).to(torch.int64) # from https://github.com/huggingface/transformers/issues/2952\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits_test = logits.detach().cpu().numpy()\n",
    "        label_ids_test = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        predictions_test.append(logits_test)\n",
    "        true_labels_test.append(label_ids_test)\n",
    "        \n",
    "    return predictions_test, true_labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_date(date_raw):\n",
    "    str_date = str(date_raw)\n",
    "    date = str_date.split(' ')\n",
    "    time = date[1]\n",
    "    date = date[0]\n",
    "    h, m = [time.split(':')[0], time.split(':')[1]]\n",
    "    DATE_TIME = date + ' ' + h + ':' + m + \" CT\"\n",
    "    print(DATE_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_validation_data(input_ids, labels, attention_masks):\n",
    "    # Split data into train, validation, test.\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = \\\n",
    "        train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n",
    "    train_masks, validation_masks, _, _ = \\\n",
    "        train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.1)\n",
    "\n",
    "    # Convert sets into Torch tensors.\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    validation_inputs = torch.tensor(validation_inputs)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    validation_labels = torch.tensor(validation_labels)\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "    # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, \n",
    "    # unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_data, train_sampler, train_dataloader, validation_data, validation_sampler, validation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_path, labels):\n",
    "    # If the model is not compiled, use PyTorch to create a pytorch_model.bin\n",
    "    path_bert = MODEL_PATH_PREFIX + model_path + '/'\n",
    "    path_bin = path_bert + 'pytorch_model.bin'\n",
    "    if (not os.path.exists(path_bin)):\n",
    "        if (model_index == 0) :\n",
    "            model_ckpt = \"bert_model.ckpt.data-00000-of-00001\"\n",
    "        else:\n",
    "            model_ckpt = \"model.ckpt-1000000\"\n",
    "        convert_tf_checkpoint_to_pytorch(path_bert + model_ckpt, \n",
    "                                         path_bert + \"bert_config.json\", \n",
    "                                         path_bert + \"pytorch_model.bin\")\n",
    "\n",
    "    # This is where the fine-tuning comes in. We must train the model for our specific task.\n",
    "    # We will first modify pre-trained BERT for our specific task, then continue training on our data until the entire model\n",
    "    # is well-suited for our task.\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_PATH_PREFIX + model_path, num_labels=len(labels))\n",
    "    model.cuda()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, learning_rate=2e-5, warmup=0.1):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters, lr=learning_rate, warmup=warmup)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, optimizer, train_data, train_sampler, train_dataloader, validation_data, validation_sampler, validation_dataloader):\n",
    "    t = [] \n",
    "    train_loss_set = [] # Store our loss and accuracy for plotting\n",
    "\n",
    "    # trange is a tqdm wrapper around the normal python range\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        # **************** Training ****************\n",
    "        # Set our model to training mode (as opposed to evaluation mode)\n",
    "        model.train()\n",
    "\n",
    "        # Tracking variables\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            b_input_ids = torch.tensor(b_input_ids).to(torch.int64) # from https://github.com/huggingface/transformers/issues/2952\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            train_loss_set.append(loss.item())    \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "        # **************** Validation ****************\n",
    "\n",
    "        # Put model in evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            b_input_ids = torch.tensor(b_input_ids).to(torch.int64) # from https://github.com/huggingface/transformers/issues/2952\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions\n",
    "                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_data(input_ids, labels, attention_masks):\n",
    "    prediction_inputs = torch.tensor(input_ids)\n",
    "    prediction_masks = torch.tensor(attention_masks)\n",
    "    prediction_labels = torch.tensor(labels)\n",
    "    prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return prediction_inputs, prediction_masks, prediction_labels, prediction_data, prediction_sampler, prediction_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_network(model, prediction_inputs, prediction_masks, prediction_labels, prediction_data, prediction_sampler, prediction_dataloader):\n",
    "    # *************************** Prediction on test set ***************************\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Set tracking variables \n",
    "    predictions_test, true_labels_test = create_predictions(prediction_dataloader, model)\n",
    "\n",
    "    # Flatten the predictions and true values\n",
    "    flat_predictions_test = [item for sublist in predictions_test for item in sublist]\n",
    "    flat_predictions_test = np.argmax(flat_predictions_test, axis=1).flatten()\n",
    "    flat_true_labels_test = [item for sublist in true_labels_test for item in sublist]\n",
    "\n",
    "    y_pred = flat_predictions_test\n",
    "    y = flat_true_labels_test\n",
    "\n",
    "    classification_dict = classification_report(y, y_pred, labels=None, target_names=None, \\\n",
    "                                                sample_weight=None, digits=2, output_dict=True, zero_division=1)\n",
    "\n",
    "    # We must alter the classification dictionary to update the f1_score and recall keys to be 1 instead of 0\n",
    "    # In this example, precision/recall/f1 scores of 0 are ignored\n",
    "    classification_dict_stripped = strip_classification_dict(true_labels_test, classification_dict)\n",
    "    \n",
    "    return classification_dict_stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_file(classification_dict_stripped, stats_file, start_date_raw, model, optimizer, model_path, \\\n",
    "                      epochs, batch_size, notes, learning_rate=2e-5):\n",
    "    global notebook_data\n",
    "    \n",
    "    ACCURACY = classification_dict_stripped['accuracy']\n",
    "\n",
    "    macro = classification_dict_stripped[\"macro avg\"]\n",
    "    MACRO_F1 = macro[\"f1-score\"]\n",
    "    MACRO_PREC = macro[\"precision\"]\n",
    "    MACRO_RECALL = macro[\"recall\"]\n",
    "    MACRO_SUPPORT = macro[\"support\"]\n",
    "\n",
    "    weighted = classification_dict_stripped[\"weighted avg\"]\n",
    "    WEIGHTED_F1 = weighted[\"f1-score\"]\n",
    "    WEIGHTED_PREC = weighted[\"precision\"]\n",
    "    WEIGHTED_RECALL = weighted[\"recall\"]\n",
    "    WEIGHTED_SUPPORT = weighted[\"support\"]\n",
    "\n",
    "    # Capture HuggingFace type\n",
    "    hf_arr = str(type(model)).split('.')\n",
    "    HF_TYPE = hf_arr[2]\n",
    "    HF_TYPE = ''.join(filter(str.isalnum, HF_TYPE))\n",
    "    HF_TYPE\n",
    "\n",
    "    # Capture optimizer type\n",
    "    opt_arr = str(type(optimizer)).split('.')\n",
    "    OPTIMIZER_TYPE = opt_arr[2]\n",
    "    OPTIMIZER_TYPE = ''.join(filter(str.isalnum, OPTIMIZER_TYPE))\n",
    "    OPTIMIZER_TYPE\n",
    "\n",
    "    date_raw = datetime.datetime.now(tz = pytz.timezone('US/Central'))\n",
    "    date = str(date_raw)\n",
    "    date = date.split(' ')\n",
    "    time = date[1]\n",
    "    date = date[0]\n",
    "    h, m = [time.split(':')[0], time.split(':')[1]]\n",
    "    DATE_TIME = date + ' ' + h + ':' + m + \" CT\"\n",
    "\n",
    "    elapsedTime = date_raw - start_date_raw\n",
    "    minutes, seconds = divmod(elapsedTime.total_seconds(), 60)\n",
    "    print(\"Model Processing Elapsed Time: minutes {}: seconds {}\".format(minutes, seconds))\n",
    "\n",
    "    NUM = int(str(getLastModelNumber(stats_file) + 1))\n",
    "\n",
    "    # Add line to stats, then save and close\n",
    "    # Create the file to store the stats of the model if it doesn't already exist\n",
    "    f = create_or_open_stats_file(STATS_PATH, stats_file)\n",
    "    f.write(\"{0},{1},{2},{3},{4},{5},{6},{7},{8},\\\n",
    "        {9},{10},{11},{12},{13},{14},{15},{16},{17},\\\n",
    "        {18},{19},{20},{21},{22},{23}\\n\".format(NUM, DATE_TIME, model_path, HF_TYPE, notebook_data.max_len, epochs, batch_size,\n",
    "                                                OPTIMIZER_TYPE, learning_rate, notebook_data.warmup, notebook_data.pretraining_model_id,\n",
    "                                                notebook_data.pretraining_model_type, notebook_data.cost_sensitivity, ACCURACY, \n",
    "                                                MACRO_PREC, MACRO_RECALL, MACRO_F1, MACRO_SUPPORT, \n",
    "                                                WEIGHTED_PREC, WEIGHTED_RECALL, WEIGHTED_F1, WEIGHTED_SUPPORT, \n",
    "                                                notebook_data.kfold, notes))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_validate_model(epochs, batch_size, notes, data_type, stats_file):\n",
    "    for model_index, model_path in enumerate(MODEL_PATH):\n",
    "        # having issues with BERT, use the others.\n",
    "        if (model_index == 0): \n",
    "            continue\n",
    "\n",
    "        # mark start time for performance evaluation\n",
    "        start_date_raw = datetime.datetime.now(tz = pytz.timezone('US/Central'))\n",
    "        print_date(start_date_raw)\n",
    "\n",
    "        # Training and Testing\n",
    "        df, sentences, tokenized_texts = get_sentences_from_file(data_type, 'big')\n",
    "        input_ids, labels, attention_masks = tokenize_sentences(get_tokenizer(), df, sentences, tokenized_texts)\n",
    "        train_data, train_sampler, train_dataloader, validation_data, validation_sampler, validation_dataloader = \\\n",
    "            get_train_validation_data(input_ids, labels, attention_masks)\n",
    "        model = get_model(model_path, labels)\n",
    "        optimizer = create_optimizer(model)\n",
    "        train_network(model, optimizer, train_data, train_sampler, train_dataloader, validation_data, validation_sampler, validation_dataloader)\n",
    "\n",
    "        # Validation\n",
    "        df, sentences, tokenized_texts = get_sentences_from_file(data_type, 'little')\n",
    "        input_ids, labels, attention_masks = tokenize_sentences(get_tokenizer(), df, sentences, tokenized_texts)\n",
    "        prediction_inputs, prediction_masks, prediction_labels, prediction_data, prediction_sampler, prediction_dataloader = \\\n",
    "            get_prediction_data(input_ids, labels, attention_masks)\n",
    "        classification_dict_stripped = predict_network(model, prediction_inputs, prediction_masks, prediction_labels, \\\n",
    "                                                       prediction_data, prediction_sampler, prediction_dataloader)\n",
    "\n",
    "        # Save to file\n",
    "        save_data_to_file(classification_dict_stripped, stats_file, start_date_raw, model, optimizer, model_path, epochs, batch_size, notes)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-20 20:52 CT\n",
      "get_sentences_from_file:Sentences found: 10045\n",
      "Removed 12 sentences greater than 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|                                                                                     | 0/2 [00:00<?, ?it/s]C:\\Users\\Keith\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09789391526080816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keith\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|██████████████████████████████████████                                      | 1/2 [04:16<04:16, 256.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0002521837074041729\n",
      "Validation Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|████████████████████████████████████████████████████████████████████████████| 2/2 [08:29<00:00, 255.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_sentences_from_file:Sentences found: 2614\n",
      "Removed 0 sentences greater than 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Keith\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Processing Elapsed Time: minutes 8.0: seconds 51.58565399999998\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'notes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b183159b7b94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0mtrain_test_validate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNOTES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDATA_TYPE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTATS_FILE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-4afa8f9d0f97>\u001b[0m in \u001b[0;36mtrain_test_validate_model\u001b[1;34m(epochs, batch_size, notes, data_type, stats_file)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Save to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0msave_data_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_dict_stripped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_date_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-369586adbe10>\u001b[0m in \u001b[0;36msave_data_to_file\u001b[1;34m(classification_dict_stripped, stats_file, start_date_raw, model, optimizer, model_path, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m     53\u001b[0m                                                 \u001b[0mMACRO_PREC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMACRO_RECALL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMACRO_F1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMACRO_SUPPORT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                                                 \u001b[0mWEIGHTED_PREC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWEIGHTED_RECALL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWEIGHTED_F1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWEIGHTED_SUPPORT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                                                 notebook_data.kfold, notes))\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'notes' is not defined"
     ]
    }
   ],
   "source": [
    "# Main processing loop\n",
    "for i, data_type in enumerate(DATA_TYPE):\n",
    "    for epochs in EPOCHS:\n",
    "        for batch_size in BATCH_SIZE:\n",
    "            train_test_validate_model(epochs, batch_size, NOTES[i], DATA_TYPE[i], STATS_FILE[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN4zHUSrA3c97XwpVIADdwZ",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Fine-Tuning Skin Cancer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
