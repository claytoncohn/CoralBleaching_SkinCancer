{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Fine Tuning Bert using Skin Cancer Data</center>\n",
    "\n",
    "The code in this notebook is adopted from: https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=IUM0UA1qJaVB\n",
    "\n",
    "Skin cancer data (big and little) can be found in the wiki: https://knowledge.depaul.edu/display/DNLP/Tasks+and+Data\n",
    "\n",
    "For this notebook, TensorFlow 1.15 is required\n",
    "\n",
    "Date: 24 August 2020\n",
    "This is a collaboration between Keith Cochran and Clayton Cohn where Skin Cancer essays can be classified.  This builds on the work from Simon Hughes involving causal reasoning chains. {Doctoral Dissertation: \"Automatic Inference of Causal Reasoning Chains from Student Essays\", 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2391,
     "status": "ok",
     "timestamp": 1587144224293,
     "user": {
      "displayName": "Clayton Cohn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiR5T0VrYZ_A0_satvSE1jZbcVxmApPyylw8Q-uxoo=s64",
      "userId": "10103672487987981310"
     },
     "user_tz": 300
    },
    "id": "pxGVOeMQOAfs",
    "outputId": "ea10eee9-a1aa-4fe0-883e-40075d3ce079"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version 1.5.1\n",
      "Tensorflow version 1.15.0\n",
      "pandas version 1.0.5.\n",
      "numpy version 1.18.5.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "from pytorch_pretrained_bert.convert_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skopt import dump\n",
    "from skopt import gp_minimize\n",
    "from skopt import load\n",
    "from skopt.plots import plot_evaluations\n",
    "from skopt.plots import plot_objective\n",
    "from skopt.space import Categorical\n",
    "from skopt.space import Integer\n",
    "from skopt.space import Real\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.set_cmap(\"viridis\")\n",
    "\n",
    "# If using Google Colab, uncomment this line to make matplotlib inline\n",
    "#% matplotlib inline\n",
    "\n",
    "print(\"Torch version {}\".format(torch.__version__))\n",
    "print(\"Tensorflow version {}\".format(tf.__version__))\n",
    "print('pandas version {}.'.format(pd.__version__))\n",
    "print('numpy version {}.'.format(np.__version__))\n",
    "\n",
    "# Get date and time\n",
    "import datetime\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>To use the GPU, do the following</center>\n",
    "\n",
    "<table><tr><th>Environment</th><th>Instruction</th></tr><tr><td>Colab</td><td>Edit->Notebook Settings->Hardware Accelerator and select GPU</td></tr>\n",
    "    <tr><td>ML PC</td><td>Device is found using the provided libraries</td></tr></table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10509,
     "status": "ok",
     "timestamp": 1587144232544,
     "user": {
      "displayName": "Clayton Cohn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiR5T0VrYZ_A0_satvSE1jZbcVxmApPyylw8Q-uxoo=s64",
      "userId": "10103672487987981310"
     },
     "user_tz": 300
    },
    "id": "HTpUC6-8UKsN",
    "outputId": "0526d979-63af-4726-f2e6-5428dae4951a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "Cuda Device: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print('Cuda Device: {}'.format(torch.cuda.get_device_name(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Manual Parameters</center>\n",
    "\n",
    "| Hyper Parameter | Recommended Values         \n",
    "| :- | :-------------\n",
    "|EPOCHS| 2, 3, 4\n",
    "|BATCH_SIZE| 16, 32\n",
    "|MAX_LEN|Length of longest sentence\n",
    "|LEARNING_RATE|2e-5, 3e-5, 5e-5\n",
    "|WARMUP|.1\n",
    "|MODEL_PATH| The path to the model to use (i.e. 'bert-base-uncased')\n",
    "|COST_SENSITIVITY|0 if unused\n",
    "|KFOLD|0 or a value for kfold cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "reOH412ibrsp"
   },
   "outputs": [],
   "source": [
    "# define the various dimensions we want to test the range of to find the optimal set.\n",
    "dimensions = [\n",
    "    Real(low=2e-5, high=5e-5, prior='log-uniform', name='learning_rate'),\n",
    "]\n",
    "\n",
    "# To modify values here, uncomment the value desired and comment the others with the same name.\n",
    "EPOCHS = [2, 3, 4]\n",
    "BATCH_SIZE = [16, 32]\n",
    "MAX_LEN = 128\n",
    "WARMUP = 0.1\n",
    "MODEL_PATH_PREFIX = 'pre-trained_models/'\n",
    "MODEL_PATH = ['bert_base_uncased', \n",
    "              'scibert_scivocab_uncased', \n",
    "              'biobert_v1.1_pubmed']\n",
    "MODEL_NAME = ['BERT', 'SCIBERT', 'BIOBERT']\n",
    "DATA_PATH = \"data\"\n",
    "STATS_PATH = \"stats\"\n",
    "PRETRAINING_MODEL_ID = \"none\"\n",
    "PRETRAINING_MODEL_TYPE = \"none\"\n",
    "COST_SENSITIVITY = 0\n",
    "KFOLD = 0\n",
    "\n",
    "DATA_TYPE =  ['skin', 'coral']\n",
    "NOTES =      ['Skin Cancer', 'Coral Bleaching']\n",
    "STATS_FILE = ['skin_cancer_fine_tuning_stats.csv', 'coral_bleaching_fine_tuning_stats.csv']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16306,
     "status": "ok",
     "timestamp": 1587144238396,
     "user": {
      "displayName": "Clayton Cohn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiR5T0VrYZ_A0_satvSE1jZbcVxmApPyylw8Q-uxoo=s64",
      "userId": "10103672487987981310"
     },
     "user_tz": 300
    },
    "id": "Y8fXOdJIULEO",
    "outputId": "85fdb01a-a3a6-49b0-cdea-a923ba4213eb"
   },
   "source": [
    "Make sure PyTorch is installed - will use with Hugging Face Transformers\n",
    "<br>Hugging Face library currently accepted as most powerful PyTorch interface with BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install\n",
    "!pip install pytorch-pretrained-bert pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Zf9TEVWWUow",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_TYPE_TO_USE = DATA_TYPE[0]\n",
    "DATA_NAME = \"\"\n",
    "\n",
    "if DATA_TYPE_TO_USE == \"skin\":\n",
    "    DATA_NAME = \"EBA1415-SkinCancer-big-sentences.tsv\"\n",
    "elif DATA_TYPE_TO_USE == \"coral\":\n",
    "    DATA_NAME = \"EBA1415-CoralBleaching-big-sentences.tsv\"\n",
    "else:\n",
    "    print(\"DATA_TYPE_TO_USE must be set to either 'coral' or 'skin.'\")\n",
    "\n",
    "h = 0 if DATA_TYPE_TO_USE == \"skin\" else None\n",
    "df = pd.read_csv(DATA_PATH + \"/\" + DATA_NAME, delimiter='\\t', header=h, names=['essay', 'relation', 's_num', 'sentence'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must transform relation labels to binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_pd = df.relation.copy(deep=True)\n",
    "\n",
    "coral_relations = [\n",
    "                   \"1,2\", \"1,3\", \"1,4\", \"1,5\", \"1,5B\", \"1,14\", \"1,6\", \"1,7\", \"1,50\",\n",
    "                   \"2,3\", \"2,4\", \"2,5\", \"2,5B\", \"2,14\", \"2,6\", \"2,7\", \"2,50\",\n",
    "                   \"3,4\", \"3,5\", \"3,5B\", \"3,14\", \"3,6\", \"3,7\", \"3,50\",\n",
    "                   \"4,5\", \"4,5B\", \"4,14\", \"4,6\", \"4,7\", \"4,50\",\n",
    "                   \"5,5B\", \"5,14\", \"5,6\", \"5,7\", \"5,50\",\n",
    "                   \"5B,14\", \"5B,6\", \"5B,7\", \"5B,50\",\n",
    "                   \"11,12\", \"11,13\", \"11,14\", \"11,6\", \"11,7\", \"11,50\",\n",
    "                   \"12,13\", \"12,14\", \"12,6\", \"12,7\", \"12,50\",\n",
    "                   \"13,14\", \"13,6\", \"13,7\",\"13,50\",\n",
    "                   \"14,6\", \"14,7\", \"14,50\",\n",
    "                   \"6,7\", \"6,50\",\n",
    "                   \"7,50\"\n",
    "                  ]\n",
    "print(\"{} unique coral bleaching relations\".format(len(coral_relations)))\n",
    "\n",
    "skin_relations = [\n",
    "                  \"1,2\", \"1,3\", \"1,4\", \"1,5\", \"1,6\", \"1,50\",\n",
    "                  \"2,3\", \"2,4\", \"2,5\", \"2,6\", \"2,50\",\n",
    "                  \"3,4\", \"3,5\", \"3,6\", \"3,50\",\n",
    "                  \"4,5\", \"4,6\", \"4,50\",\n",
    "                  \"5,6\", \"5,50\",\n",
    "                  \"11,12\", \"11,6\", \"11,50\",\n",
    "                  \"12,6\", \"12,50\",\n",
    "                  \"6,50\"     \n",
    "                 ]\n",
    "\n",
    "print(\"{} unique skin cancer relations\".format(len(skin_relations)))\n",
    "\n",
    "for i, rel in relations_pd.items():\n",
    "    chain = rel.split(\"-\")\n",
    "    if chain[0] != \"O\":\n",
    "        chain = chain[1] + \",\" + chain[2]\n",
    "        \n",
    "        if DATA_TYPE == \"coral\":\n",
    "            if chain in coral_relations:\n",
    "                relations_pd.at[i] = 1\n",
    "                continue\n",
    "\n",
    "        elif DATA_TYPE == \"skin\":\n",
    "            if chain in skin_relations:\n",
    "                relations_pd.at[i] = 1\n",
    "                continue\n",
    "            \n",
    "    relations_pd.at[i] = 0\n",
    "\n",
    "df_binary = df.copy(deep=True)\n",
    "df_binary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary.relation = relations_pd\n",
    "df_binary.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must address the issue that some sentences have multiple relations. This could be a problem if a sentence has one valid relation and one invalid one (the same sentence will be labeled True in one instance and False in another instance). To correct this, we will remove the duplicate instances and define each sentence to be True if it contains *at least one* causal relation.\n",
    "\n",
    "The parse was provided by @TrentonMcKinney on StackOverflow:\n",
    "https://stackoverflow.com/questions/63697275/regex-string-for-different-versions/63697498#63697498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicate_sentences = df_binary[df_binary.s_num.astype(str).str.split('.', expand=True)[1] != '0']\n",
    "df_duplicate_sentences.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the duplicates are isolated, they need to be evaluated. If there is at least one relation, one copy of the sentence will be kept as true. If there are no relations, one copy will be kept as false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = -1\n",
    "same_arr_inds = []\n",
    "drop_list = []\n",
    "\n",
    "for i, row in df_duplicate_sentences.iterrows():\n",
    "    s_num = str(df_duplicate_sentences.loc[i].s_num)\n",
    "    first_num, second_num = s_num.split(\".\")\n",
    "\n",
    "    if first_num != current:\n",
    "        current = first_num\n",
    "\n",
    "    if len(same_arr_inds) > 1:\n",
    "        flag = False\n",
    "        for n in same_arr_inds:\n",
    "            if df_duplicate_sentences.loc[n].relation == True:\n",
    "                flag = True\n",
    "                break\n",
    "\n",
    "        left = same_arr_inds[0]\n",
    "        right = same_arr_inds[1:]\n",
    "\n",
    "        if flag == True:\n",
    "            df_duplicate_sentences.loc[left].relation = 1\n",
    "        else:\n",
    "            df_duplicate_sentences.loc[left].relation = 0\n",
    "\n",
    "        drop_list += right   \n",
    "\n",
    "        same_arr_inds = []\n",
    "        same_arr_inds.append(i)\n",
    "\n",
    "df_binary.drop(drop_list, inplace=True)   \n",
    "df_binary.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is prepped and cleaned at this point. Next is implementation. Extract sentences and labels from DataFrame. Must also add special [CLS] and [SEP] tokens for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sentences for BERT.\n",
    "For each tokenized input sentence, we need to create:\n",
    "\n",
    "1. input ids:\n",
    "    a sequence of integers identifying each input token to its index number \n",
    "    in the BERT tokenizer vocabulary\n",
    "\n",
    "2. segment mask: (optional) a sequence of 1s and 0s used to identify whether the input is one \n",
    "    sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. \n",
    "    For two sentence inputs, there is a 0 for each token of the first sentence, followed by a \n",
    "    1 for each token of the second sentence\n",
    "\n",
    "3. attention mask: (optional) \n",
    "    a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens \n",
    "\n",
    "4. labels: based on the labels from the data set\n",
    "\n",
    "Additionally, we will get rid of the sentences greater than MAX_LEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(tokenizer, sentences):\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    print (\"First sentence tokenized: \", tokenized_texts[0])\n",
    "    \n",
    "    original_length = len(tokenized_texts)\n",
    "    labels = df_binary.relation.values\n",
    "    labels = [labels[i] for i in range(len(tokenized_texts)) if len(tokenized_texts[i]) <= MAX_LEN]\n",
    "    tokenized_texts = [tokenized_texts[i] for i in range(len(tokenized_texts)) if len(tokenized_texts[i]) <= MAX_LEN]\n",
    "    print(\"Removed {0} sentences greater than {1}\".format(original_length - len(tokenized_texts),MAX_LEN))\n",
    "    \n",
    "    # Convert BERT tokens to corresponding ID numbers in BERT vocabulary. After conversion, pad the sequences.\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    # Create attention masks.\n",
    "    attention_masks = []\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    return input_ids, labels, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GANGNKH7jbd2"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqqQCDEWs0oN"
   },
   "outputs": [],
   "source": [
    "def getLastModelNumber(stats_file):\n",
    "    try:\n",
    "        with open(STATS_PATH + \"/\" + stats_file, \"r\") as f:\n",
    "            f_list = list(f)\n",
    "            latest = f_list[-1].split(',')\n",
    "            return int(latest[0])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_validate_model(epochs, batch_size, notes, data_type, stats_file, sentences, tokenizer):\n",
    "    for model_index, model_path in enumerate(MODEL_PATH):\n",
    "        # having issues with BERT, use the others.\n",
    "        if (model_index == 0): \n",
    "            continue\n",
    "            \n",
    "        start_date_raw = datetime.datetime.now(tz = pytz.timezone('US/Central'))\n",
    "        start_date = str(start_date_raw)\n",
    "        date = start_date.split(' ')\n",
    "        time = date[1]\n",
    "        date = date[0]\n",
    "        h, m = [time.split(':')[0], time.split(':')[1]]\n",
    "\n",
    "        DATE_TIME = date + ' ' + h + ':' + m + \" CT\"\n",
    "        #print(DATE_TIME)\n",
    "\n",
    "        input_ids, labels, attention_masks = tokenize_sentences(tokenizer, sentences)\n",
    "\n",
    "        # Split data into train, validation, test.\n",
    "        train_inputs, validation_inputs, train_labels, validation_labels = \\\n",
    "            train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n",
    "        train_masks, validation_masks, _, _ = \\\n",
    "            train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.1)\n",
    "\n",
    "        # Convert sets into Torch tensors.\n",
    "        train_inputs = torch.tensor(train_inputs)\n",
    "        validation_inputs = torch.tensor(validation_inputs)\n",
    "        train_labels = torch.tensor(train_labels)\n",
    "        validation_labels = torch.tensor(validation_labels)\n",
    "        train_masks = torch.tensor(train_masks)\n",
    "        validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "        # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, \n",
    "        # unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "        train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "        validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "        validation_sampler = SequentialSampler(validation_data)\n",
    "        validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "        # If the model is not compiled, use PyTorch to create a pytorch_model.bin\n",
    "        path_bert = MODEL_PATH_PREFIX + model_path + '/'\n",
    "        path_bin = path_bert + 'pytorch_model.bin'\n",
    "        if (not os.path.exists(path_bin)):\n",
    "            if (model_index == 0) :\n",
    "                model_ckpt = \"bert_model.ckpt.data-00000-of-00001\"\n",
    "            else:\n",
    "                model_ckpt = \"model.ckpt-1000000\"\n",
    "            convert_tf_checkpoint_to_pytorch(path_bert + model_ckpt, \n",
    "                                             path_bert + \"bert_config.json\", \n",
    "                                             path_bert + \"pytorch_model.bin\")\n",
    "\n",
    "        # This is where the fine-tuning comes in. We must train the model for our specific task.\n",
    "        # We will first modify pre-trained BERT for our specific task, then continue training on our data until the entire model\n",
    "        # is well-suited for our task.\n",
    "        model = BertForSequenceClassification.from_pretrained(MODEL_PATH_PREFIX + model_path, num_labels=len(labels))\n",
    "        model.cuda()\n",
    "\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "        optimizer = BertAdam(optimizer_grouped_parameters, lr=2e-5, warmup=WARMUP)\n",
    "\n",
    "        t = [] \n",
    "        train_loss_set = [] # Store our loss and accuracy for plotting\n",
    "\n",
    "        # trange is a tqdm wrapper around the normal python range\n",
    "        for _ in trange(epochs, desc=\"Epoch\"):\n",
    "            # **************** Training ****************\n",
    "            # Set our model to training mode (as opposed to evaluation mode)\n",
    "            model.train()\n",
    "\n",
    "            # Tracking variables\n",
    "            tr_loss = 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "            # Train the data for one epoch\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                # Add batch to GPU\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                # Unpack the inputs from our dataloader\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "                b_input_ids = torch.tensor(b_input_ids).to(torch.int64) # from https://github.com/huggingface/transformers/issues/2952\n",
    "                # Clear out the gradients (by default they accumulate)\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "                train_loss_set.append(loss.item())    \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                # Update parameters and take a step using the computed gradient\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update tracking variables\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_examples += b_input_ids.size(0)\n",
    "                nb_tr_steps += 1\n",
    "\n",
    "            print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "            # **************** Validation ****************\n",
    "\n",
    "            # Put model in evaluation mode to evaluate loss on the validation set\n",
    "            model.eval()\n",
    "\n",
    "            # Tracking variables \n",
    "            eval_loss, eval_accuracy = 0, 0\n",
    "            nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "            # Evaluate data for one epoch\n",
    "            for batch in validation_dataloader:\n",
    "                # Add batch to GPU\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                # Unpack the inputs from our dataloader\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "                b_input_ids = torch.tensor(b_input_ids).to(torch.int64) # from https://github.com/huggingface/transformers/issues/2952\n",
    "                # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "                with torch.no_grad():\n",
    "                    # Forward pass, calculate logit predictions\n",
    "                    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "                # Move logits and labels to CPU\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "                tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "                eval_accuracy += tmp_eval_accuracy\n",
    "                nb_eval_steps += 1\n",
    "\n",
    "            print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "        # Now we will validate on our testing data\n",
    "        if DATA_TYPE_TO_USE == \"skin\":\n",
    "            DATA_NAME = \"EBA1415-SkinCancer-little-sentences.tsv\"\n",
    "        elif DATA_TYPE_TO_USER == \"coral\":\n",
    "            DATA_NAME = \"EBA1415-CoralBleaching-little-sentences.tsv\"\n",
    "        else:\n",
    "            print(\"DATA_TYPE_TO_USER must be set to either 'coral' or 'skin'\")\n",
    "\n",
    "        h = 0 if DATA_TYPE == DATA_TYPE_TO_USE else None\n",
    "        df_test = pd.read_csv(DATA_PATH  + \"/\" + DATA_NAME, delimiter='\\t', header=h, names=['essay', 'relation', 's_num', 'sentence'])\n",
    "\n",
    "        # Create sentence and label lists\n",
    "        sentences_test = df_test.sentence.values\n",
    "\n",
    "        # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "        sentences_test = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences_test]\n",
    "\n",
    "        tokenized_texts_test = [tokenizer.tokenize(sentence) for sentence in sentences_test]\n",
    "\n",
    "        # Get labels\n",
    "        input_ids_test, labels_test, attention_masks_test = tokenize_sentences(tokenizer, sentences_test)\n",
    "\n",
    "        prediction_inputs = torch.tensor(input_ids_test)\n",
    "        prediction_masks = torch.tensor(attention_masks_test)\n",
    "        prediction_labels = torch.tensor(labels_test)\n",
    "        prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "        prediction_sampler = SequentialSampler(prediction_data)\n",
    "        prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "        # *************************** Prediction on test set ***************************\n",
    "        # Put model in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        predictions_test, true_labels_test = [], []\n",
    "\n",
    "        # Predict \n",
    "        for batch in prediction_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            b_input_ids = torch.tensor(b_input_ids).to(torch.int64) # from https://github.com/huggingface/transformers/issues/2952\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions\n",
    "                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits_test = logits.detach().cpu().numpy()\n",
    "            label_ids_test = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            predictions_test.append(logits_test)\n",
    "            true_labels_test.append(label_ids_test)\n",
    "\n",
    "\n",
    "        # Flatten the predictions and true values\n",
    "        flat_predictions_test = [item for sublist in predictions_test for item in sublist]\n",
    "        flat_predictions_test = np.argmax(flat_predictions_test, axis=1).flatten()\n",
    "        flat_true_labels_test = [item for sublist in true_labels_test for item in sublist]\n",
    "\n",
    "        # Create the file to store the stats of the model if it doesn't already exist\n",
    "        f = None\n",
    "        if not os.path.isfile(STATS_PATH + \"/\" + stats_file):\n",
    "            f = open(STATS_PATH + \"/\" + stats_file, \"w\")\n",
    "            f.write(\"number,datetime,bert_model,hugging_face,max_len,epochs,batch_size,\\\n",
    "                optimizer,learning_rate,warmup,pretraining_model_id,pretraining_model_type,\\\n",
    "                cost_sensitivity,accuracy,macro_prec,macro_recall,macro_f1,macro_support,\\\n",
    "                weighted_prec,weighted_recall,weighted_f1,weighted_support,kfold,notes\\n\")\n",
    "            print(\"skin_cancer_stats.csv NOT found - creating\")\n",
    "            f.close()\n",
    "\n",
    "        y_pred = flat_predictions_test\n",
    "        y = flat_true_labels_test\n",
    "\n",
    "        classification_dict = classification_report(y, y_pred, labels=None, target_names=None, \\\n",
    "                                                    sample_weight=None, digits=2, output_dict=True, zero_division=1)\n",
    "\n",
    "        # Create arrays of precisions, recalls, f1s to recalculate average\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "        supports = []\n",
    "\n",
    "        # We must alter the classification dictionary to update the f1_score and recall keys to be 1 instead of 0\n",
    "        # In this example, precision/recall/f1 scores of 0 are ignored\n",
    "\n",
    "        classification_dict_stripped = {}\n",
    "\n",
    "        for k,v in classification_dict.items():\n",
    "            if k.isdigit():\n",
    "                if float(v['precision']) != 0.0 and float(v['recall']) != 0.0 and float(v['f1-score']) != 0.0:\n",
    "                    recalls.append(v['recall'])\n",
    "                    f1s.append(v['f1-score'])\n",
    "                    precisions.append(v['precision'])\n",
    "                    supports.append(v['support'])\n",
    "\n",
    "                    # Convert dictionary keys back to our original labels\n",
    "                    original_key = next((key for key in label_types if label_types[key] == int(k)), None)\n",
    "                    classification_dict_stripped.update({ original_key : v })\n",
    "\n",
    "            # Otherwise, we are at the end of the dict and edit the averages to account for the newly replaced 0s\n",
    "            else:\n",
    "                if k == 'macro avg':\n",
    "                    precision = sum(precisions)/len(precisions)\n",
    "                    recall = sum(recalls)/len(recalls)\n",
    "                    f1 = sum(f1s)/len(f1s)\n",
    "\n",
    "                    v['precision'] = precision\n",
    "                    v['recall'] = recall\n",
    "                    v['f1-score'] = f1\n",
    "\n",
    "                if k == 'weighted avg':\n",
    "                    weighted_precisions = [precisions[i]*supports[i] for i in range(len(precisions))]\n",
    "                    weighted_recalls = [recalls[i]*supports[i] for i in range(len(recalls))]\n",
    "                    weighted_f1s = [f1s[i]*supports[i] for i in range(len(f1s))]\n",
    "\n",
    "                    total_supports = sum(supports)\n",
    "\n",
    "                    precision = sum(weighted_precisions)/total_supports\n",
    "                    recall = sum(weighted_recalls)/total_supports\n",
    "                    f1 = sum(weighted_f1s)/total_supports\n",
    "\n",
    "                v['precision'] = precision\n",
    "                v['recall'] = recall\n",
    "                v['f1-score'] = f1\n",
    "                v['support'] = total_supports\n",
    "\n",
    "            classification_dict_stripped.update({ k : v })\n",
    "\n",
    "        ACCURACY = classification_dict_stripped['accuracy']\n",
    "\n",
    "        macro = classification_dict_stripped[\"macro avg\"]\n",
    "        MACRO_F1 = macro[\"f1-score\"]\n",
    "        MACRO_PREC = macro[\"precision\"]\n",
    "        MACRO_RECALL = macro[\"recall\"]\n",
    "        MACRO_SUPPORT = macro[\"support\"]\n",
    "\n",
    "        weighted = classification_dict_stripped[\"weighted avg\"]\n",
    "        WEIGHTED_F1 = weighted[\"f1-score\"]\n",
    "        WEIGHTED_PREC = weighted[\"precision\"]\n",
    "        WEIGHTED_RECALL = weighted[\"recall\"]\n",
    "        WEIGHTED_SUPPORT = weighted[\"support\"]\n",
    "\n",
    "        # Capture HuggingFace type\n",
    "        hf_arr = str(type(model)).split('.')\n",
    "        HF_TYPE = hf_arr[2]\n",
    "        HF_TYPE = ''.join(filter(str.isalnum, HF_TYPE))\n",
    "        HF_TYPE\n",
    "\n",
    "        # Capture optimizer type\n",
    "        opt_arr = str(type(optimizer)).split('.')\n",
    "        OPTIMIZER_TYPE = opt_arr[2]\n",
    "        OPTIMIZER_TYPE = ''.join(filter(str.isalnum, OPTIMIZER_TYPE))\n",
    "        OPTIMIZER_TYPE\n",
    "\n",
    "        date_raw = datetime.datetime.now(tz = pytz.timezone('US/Central'))\n",
    "        date = str(date_raw)\n",
    "        date = date.split(' ')\n",
    "        time = date[1]\n",
    "        date = date[0]\n",
    "        h, m = [time.split(':')[0], time.split(':')[1]]\n",
    "\n",
    "        DATE_TIME = date + ' ' + h + ':' + m + \" CT\"\n",
    "        print(DATE_TIME)\n",
    "\n",
    "        elapsedTime = date_raw - start_date_raw\n",
    "        minutes, seconds = divmod(elapsedTime.total_seconds(), 60)\n",
    "        print(\"minutes {}: seconds {}\".format(minutes, seconds))\n",
    "\n",
    "        NUM = int(current_file_n_str)\n",
    "\n",
    "        # Add line to stats, then save and close\n",
    "        with open(STATS_PATH + \"/\" + stats_file, \"a\") as f:\n",
    "            f.write(\"{0},{1},{2},{3},{4},{5},{6},{7},{8},\\\n",
    "            {9},{10},{11},{12},{13},{14},{15},{16},{17},\\\n",
    "            {18},{19},{20},{21},{22},{23}\\n\".format(NUM,DATE_TIME,model_path,HF_TYPE,MAX_LEN,epocs,batch_size,\n",
    "                                                  OPTIMIZER_TYPE,LEARNING_RATE,WARMUP,PRETRAINING_MODEL_ID,PRETRAINING_MODEL_TYPE,\n",
    "                                                  COST_SENSITIVITY,ACCURACY,MACRO_PREC,MACRO_RECALL,MACRO_F1,MACRO_SUPPORT,\n",
    "                                                  WEIGHTED_PREC,WEIGHTED_RECALL,WEIGHTED_F1,WEIGHTED_SUPPORT,KFOLD,notes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_binary.sentence.values\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Main processing loop\n",
    "for i, data_type in enumerate(DATA_TYPE):\n",
    "    for epochs in EPOCHS:\n",
    "        for batch_size in BATCH_SIZE:\n",
    "            train_test_validate_model(epochs, batch_size, NOTES[i], DATA_TYPE[i], STATS_FILE[i], sentences, tokenizer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN4zHUSrA3c97XwpVIADdwZ",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Fine-Tuning Skin Cancer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
